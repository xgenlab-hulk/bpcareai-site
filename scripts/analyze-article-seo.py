#!/usr/bin/env python3
"""
æ–‡ç«  SEO è´¨é‡åˆ†æè„šæœ¬
åˆ†ææ‰€æœ‰æ–‡ç« çš„è´¨é‡ã€ç»“æ„ç›¸ä¼¼æ€§ã€AIç—•è¿¹ç­‰
"""

import os
import re
import json
from collections import Counter, defaultdict
from pathlib import Path
import frontmatter

# AI é«˜é¢‘ç‰¹å¾è¯
AI_MARKERS = [
    "it's important to note",
    "it is important to note",
    "moreover",
    "delve",
    "navigating",
    "here's what you can do",
    "who should pay special attention",
    "final thoughts: a connected approach",
    "doesn't require drastic changes",
    "small shifts",
]

def read_article(file_path):
    """è¯»å–æ–‡ç« å¹¶è§£æ"""
    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            post = frontmatter.load(f)
            return {
                'slug': post.get('slug', Path(file_path).stem),
                'title': post.get('title', ''),
                'description': post.get('description', ''),
                'primaryKeyword': post.get('primaryKeyword', ''),
                'topicCluster': post.get('topicCluster', ''),
                'tags': post.get('tags', []),
                'relatedSlugs': post.get('relatedSlugs', []),
                'content': post.content,
            }
    except Exception as e:
        print(f"é”™è¯¯è¯»å– {file_path}: {e}")
        return None

def extract_headings(content):
    """æå– H2 å’Œ H3 æ ‡é¢˜"""
    h2_pattern = r'^##\s+(.+)$'
    h3_pattern = r'^###\s+(.+)$'

    h2_headings = re.findall(h2_pattern, content, re.MULTILINE)
    h3_headings = re.findall(h3_pattern, content, re.MULTILINE)

    return {
        'h2': h2_headings,
        'h3': h3_headings,
        'h2_count': len(h2_headings),
        'h3_count': len(h3_headings),
    }

def calculate_similarity(headings1, headings2):
    """è®¡ç®—ä¸¤ç¯‡æ–‡ç« H2æ ‡é¢˜çš„ç›¸ä¼¼åº¦ï¼ˆ0-100ï¼‰"""
    if not headings1 or not headings2:
        return 0

    # ç®€åŒ–æ ‡é¢˜ï¼ˆå»é™¤å…·ä½“å†…å®¹ï¼Œä¿ç•™ç»“æ„è¯ï¼‰
    def simplify(heading):
        heading = heading.lower()
        # ä¿ç•™ç»“æ„æ€§è¯æ±‡
        structure_words = ['why', 'how', 'what', 'when', 'practical', 'steps',
                          'final', 'thoughts', 'faq', 'understanding', 'science']
        words = [w for w in heading.split() if w in structure_words or len(w) <= 4]
        return ' '.join(words)

    simple1 = [simplify(h) for h in headings1]
    simple2 = [simplify(h) for h in headings2]

    # è®¡ç®—äº¤é›†æ¯”ä¾‹
    if not simple1 or not simple2:
        return 0

    matches = sum(1 for s1 in simple1 for s2 in simple2 if s1 and s2 and s1 == s2)
    avg_len = (len(simple1) + len(simple2)) / 2

    return int((matches / avg_len) * 100) if avg_len > 0 else 0

def detect_ai_markers(content):
    """æ£€æµ‹ AI ç—•è¿¹"""
    content_lower = content.lower()
    found_markers = []

    for marker in AI_MARKERS:
        if marker in content_lower:
            count = content_lower.count(marker)
            found_markers.append((marker, count))

    return found_markers

def count_words(content):
    """ç»Ÿè®¡å­—æ•°ï¼ˆè‹±æ–‡å•è¯ï¼‰"""
    words = re.findall(r'\b\w+\b', content)
    return len(words)

def calculate_keyword_density(content, keyword):
    """è®¡ç®—å…³é”®è¯å¯†åº¦"""
    if not keyword:
        return 0

    content_lower = content.lower()
    keyword_lower = keyword.lower()

    keyword_count = content_lower.count(keyword_lower)
    total_words = count_words(content)

    if total_words == 0:
        return 0

    return (keyword_count / total_words) * 100

def score_article(article, headings, all_articles_headings):
    """ä¸ºæ–‡ç« è¯„åˆ†"""
    scores = {
        'seo_tech': 0,      # SEOæŠ€æœ¯å…ƒç´  /20
        'content': 0,       # å†…å®¹è´¨é‡ /30
        'keyword': 0,       # å…³é”®è¯ä¼˜åŒ– /15
        'ux': 0,           # ç”¨æˆ·ä½“éªŒ /15
        'differentiation': 0,  # å·®å¼‚åŒ– /10
        'ai_penalty': 0,   # AIç—•è¿¹æ‰£åˆ† /10
    }

    issues = []

    # === SEO æŠ€æœ¯å…ƒç´ è¯„åˆ† (20åˆ†) ===
    # æ ‡é¢˜é•¿åº¦ (4åˆ†)
    title_len = len(article['title'])
    if 50 <= title_len <= 70:
        scores['seo_tech'] += 4
    elif 40 <= title_len <= 80:
        scores['seo_tech'] += 2
    else:
        issues.append(f"æ ‡é¢˜é•¿åº¦ä¸ä½³({title_len}å­—ç¬¦)")

    # Metaæè¿° (4åˆ†)
    desc_len = len(article['description'])
    if 140 <= desc_len <= 170:
        scores['seo_tech'] += 4
    elif 120 <= desc_len <= 180:
        scores['seo_tech'] += 2
    else:
        issues.append(f"æè¿°é•¿åº¦ä¸ä½³({desc_len}å­—ç¬¦)")

    # å…³é”®è¯ç­–ç•¥ (4åˆ†)
    if article['primaryKeyword']:
        scores['seo_tech'] += 2
        if article['primaryKeyword'].lower() in article['title'].lower():
            scores['seo_tech'] += 2
        else:
            issues.append("æ ‡é¢˜æœªåŒ…å«ä¸»å…³é”®è¯")
    else:
        issues.append("ç¼ºå°‘ä¸»å…³é”®è¯")

    # å†…é“¾ (4åˆ†)
    related_count = len(article['relatedSlugs'])
    if related_count >= 3:
        scores['seo_tech'] += 4
    elif related_count >= 1:
        scores['seo_tech'] += 2
    else:
        issues.append("ç¼ºå°‘å†…é“¾")

    # æ ‡ç­¾ (4åˆ†)
    tags_count = len(article['tags'])
    if 2 <= tags_count <= 5:
        scores['seo_tech'] += 4
    elif tags_count >= 1:
        scores['seo_tech'] += 2
    else:
        issues.append("ç¼ºå°‘æ ‡ç­¾")

    # === å†…å®¹è´¨é‡è¯„åˆ† (30åˆ†) ===
    word_count = count_words(article['content'])

    # æ–‡ç« é•¿åº¦ (5åˆ†)
    if 1200 <= word_count <= 2500:
        scores['content'] += 5
    elif 800 <= word_count <= 3000:
        scores['content'] += 3
    elif word_count < 800:
        scores['content'] += 1
        issues.append(f"å†…å®¹è¿‡çŸ­({word_count}è¯)")

    # å†…å®¹ç»“æ„ (5åˆ†)
    if 4 <= headings['h2_count'] <= 7:
        scores['content'] += 5
    elif headings['h2_count'] >= 3:
        scores['content'] += 3
    else:
        issues.append(f"H2æ ‡é¢˜è¿‡å°‘({headings['h2_count']}ä¸ª)")

    # ä¸“ä¸šæ·±åº¦ (10åˆ†)
    content_lower = article['content'].lower()
    has_research = any(word in content_lower for word in ['study', 'research', 'published', 'journal'])
    has_data = '%' in article['content'] or any(word in content_lower for word in ['statistics', 'data'])
    has_citations = 'american heart association' in content_lower or 'american college' in content_lower

    if has_research:
        scores['content'] += 4
    if has_data:
        scores['content'] += 3
    if has_citations:
        scores['content'] += 3

    if not (has_research or has_data):
        issues.append("ç¼ºå°‘ç ”ç©¶å¼•ç”¨/æ•°æ®æ”¯æŒ")

    # å®ç”¨æ€§ (10åˆ†)
    has_steps = any(word in content_lower for word in ['steps', 'tips', 'how to', 'what you can do'])
    has_examples = any(word in content_lower for word in ['example', 'for instance'])
    has_faq = 'faq' in content_lower or '####' in article['content']

    if has_steps:
        scores['content'] += 4
    if has_examples:
        scores['content'] += 3
    if has_faq:
        scores['content'] += 3
    else:
        issues.append("ç¼ºå°‘FAQéƒ¨åˆ†")

    # === å…³é”®è¯ä¼˜åŒ– (15åˆ†) ===
    if article['primaryKeyword']:
        density = calculate_keyword_density(article['content'], article['primaryKeyword'])

        # å¯†åº¦è¯„åˆ† (5åˆ†)
        if 1.0 <= density <= 2.0:
            scores['keyword'] += 5
        elif 0.5 <= density <= 2.5:
            scores['keyword'] += 3
        elif density < 0.5:
            scores['keyword'] += 1
            issues.append(f"å…³é”®è¯å¯†åº¦è¿‡ä½({density:.2f}%)")
        else:
            issues.append(f"å…³é”®è¯å¯†åº¦è¿‡é«˜({density:.2f}%)")

        # ä½ç½®åˆ†å¸ƒ (7åˆ†)
        first_100_words = ' '.join(article['content'].split()[:100]).lower()
        keyword_lower = article['primaryKeyword'].lower()

        if keyword_lower in article['title'].lower():
            scores['keyword'] += 3
        if keyword_lower in first_100_words:
            scores['keyword'] += 2
        if any(keyword_lower in h.lower() for h in headings['h2']):
            scores['keyword'] += 2

        # LSIå…³é”®è¯ (3åˆ†) - ç®€åŒ–æ£€æµ‹
        scores['keyword'] += 3  # é»˜è®¤ç»™åˆ†

    # === ç”¨æˆ·ä½“éªŒ (15åˆ†) ===
    # å¯è¯»æ€§ (8åˆ†) - ç®€åŒ–è¯„ä¼°
    avg_words_per_para = word_count / max(article['content'].count('\n\n'), 1)
    if avg_words_per_para < 100:
        scores['ux'] += 8
    else:
        scores['ux'] += 4

    # æ ¼å¼å‹å¥½ (7åˆ†)
    has_lists = article['content'].count('\n- ') + article['content'].count('\n* ')
    if has_lists >= 5:
        scores['ux'] += 7
    elif has_lists >= 2:
        scores['ux'] += 4

    # === å·®å¼‚åŒ–è¯„åˆ† (10åˆ†) ===
    # è®¡ç®—ä¸å…¶ä»–æ–‡ç« çš„å¹³å‡ç›¸ä¼¼åº¦
    similarities = []
    for other_slug, other_headings in all_articles_headings.items():
        if other_slug != article['slug']:
            sim = calculate_similarity(headings['h2'], other_headings['h2'])
            similarities.append(sim)

    avg_similarity = sum(similarities) / len(similarities) if similarities else 0

    # ç›¸ä¼¼åº¦è¶Šä½ï¼Œå·®å¼‚åŒ–åˆ†æ•°è¶Šé«˜
    if avg_similarity < 40:
        scores['differentiation'] += 10
    elif avg_similarity < 60:
        scores['differentiation'] += 7
    elif avg_similarity < 75:
        scores['differentiation'] += 4
    else:
        issues.append(f"ç»“æ„é«˜åº¦æ¨¡æ¿åŒ–(ç›¸ä¼¼åº¦{avg_similarity:.0f}%)")

    # === AI ç—•è¿¹æ‰£åˆ† (10åˆ†) ===
    ai_markers = detect_ai_markers(article['content'])
    ai_score = min(len(ai_markers), 10)
    scores['ai_penalty'] = ai_score

    if ai_score >= 5:
        issues.append(f"AIç—•è¿¹æ˜æ˜¾({len(ai_markers)}ä¸ªç‰¹å¾)")

    # è®¡ç®—æ€»åˆ†
    total = (scores['seo_tech'] + scores['content'] + scores['keyword'] +
             scores['ux'] + scores['differentiation'] - scores['ai_penalty'])

    return {
        'scores': scores,
        'total': total,
        'issues': issues,
        'word_count': word_count,
        'ai_markers': ai_markers,
        'avg_similarity': avg_similarity,
    }

def main():
    print("å¼€å§‹åˆ†æ 84 ç¯‡æ–‡ç« ...")

    articles_dir = Path('content/articles')
    article_files = list(articles_dir.glob('*.md'))

    print(f"æ‰¾åˆ° {len(article_files)} ç¯‡æ–‡ç« ")

    # ç¬¬ä¸€è½®ï¼šè¯»å–æ‰€æœ‰æ–‡ç« 
    articles = []
    all_headings = {}

    for i, file_path in enumerate(article_files, 1):
        if i % 10 == 0:
            print(f"è¿›åº¦: {i}/{len(article_files)}")

        article = read_article(file_path)
        if article:
            headings = extract_headings(article['content'])
            article['headings'] = headings
            articles.append(article)
            all_headings[article['slug']] = headings

    print(f"æˆåŠŸè¯»å– {len(articles)} ç¯‡æ–‡ç« ")

    # ç¬¬äºŒè½®ï¼šè¯„åˆ†
    print("\nå¼€å§‹è¯„åˆ†åˆ†æ...")
    results = []

    for i, article in enumerate(articles, 1):
        if i % 10 == 0:
            print(f"è¯„åˆ†è¿›åº¦: {i}/{len(articles)}")

        result = score_article(article, article['headings'], all_headings)
        results.append({
            'article': article,
            'result': result,
        })

    # ç”ŸæˆæŠ¥å‘Š
    print("\nç”Ÿæˆåˆ†ææŠ¥å‘Š...")
    generate_report(results, all_headings)
    generate_csv(results)

    print("\nâœ… åˆ†æå®Œæˆï¼")
    print("ç”Ÿæˆæ–‡ä»¶:")
    print("  - article-seo-analysis-report.md")
    print("  - article-seo-scores.csv")

def generate_report(results, all_headings):
    """ç”Ÿæˆä¸­æ–‡ Markdown æŠ¥å‘Š"""
    # æŒ‰æ€»åˆ†æ’åº
    results.sort(key=lambda x: x['result']['total'], reverse=True)

    total_articles = len(results)
    avg_score = sum(r['result']['total'] for r in results) / total_articles

    excellent = [r for r in results if r['result']['total'] >= 80]
    good = [r for r in results if 70 <= r['result']['total'] < 80]
    needs_improve = [r for r in results if 60 <= r['result']['total'] < 70]
    poor = [r for r in results if r['result']['total'] < 60]

    report = []
    report.append("# 84ç¯‡æ–‡ç«  SEO è´¨é‡åˆ†ææŠ¥å‘Š\n")
    report.append(f"**åˆ†ææ—¥æœŸ**: {__import__('datetime').date.today()}\n")
    report.append(f"**åˆ†ææ–‡ç« æ•°**: {total_articles} ç¯‡\n\n")

    # ä¸€ã€æ‰§è¡Œæ‘˜è¦
    report.append("## ä¸€ã€æ‰§è¡Œæ‘˜è¦\n\n")
    report.append(f"- **å¹³å‡è´¨é‡è¯„åˆ†**: {avg_score:.1f}/100\n")
    report.append(f"- **ä¼˜ç§€æ–‡ç« ** (â‰¥80åˆ†): {len(excellent)}ç¯‡ ({len(excellent)/total_articles*100:.1f}%)\n")
    report.append(f"- **è‰¯å¥½æ–‡ç« ** (70-79åˆ†): {len(good)}ç¯‡ ({len(good)/total_articles*100:.1f}%)\n")
    report.append(f"- **éœ€ä¼˜åŒ–æ–‡ç« ** (60-69åˆ†): {len(needs_improve)}ç¯‡ ({len(needs_improve)/total_articles*100:.1f}%)\n")
    report.append(f"- **é—®é¢˜æ–‡ç« ** (<60åˆ†): {len(poor)}ç¯‡ ({len(poor)/total_articles*100:.1f}%)\n\n")

    # æ ¸å¿ƒå‘ç°
    report.append("### æ ¸å¿ƒå‘ç°\n\n")
    avg_similarity = sum(r['result']['avg_similarity'] for r in results) / total_articles
    high_ai = [r for r in results if r['result']['scores']['ai_penalty'] >= 5]
    no_internal_links = [r for r in results if len(r['article']['relatedSlugs']) == 0]

    report.append(f"1. **ç»“æ„åŒè´¨åŒ–ç¨‹åº¦**: å¹³å‡ç›¸ä¼¼åº¦ {avg_similarity:.1f}%\n")
    report.append(f"2. **AIç—•è¿¹æ˜æ˜¾**: {len(high_ai)}ç¯‡æ–‡ç«  ({len(high_ai)/total_articles*100:.1f}%)\n")
    report.append(f"3. **ç¼ºå°‘å†…é“¾**: {len(no_internal_links)}ç¯‡æ–‡ç« \n\n")

    # äºŒã€ç»“æ„ç›¸ä¼¼æ€§åˆ†æ
    report.append("## äºŒã€ç»“æ„ç›¸ä¼¼æ€§åˆ†æ\n\n")

    # è¯†åˆ«å¸¸è§H2æ¨¡å¼
    h2_patterns = Counter()
    for slug, headings in all_headings.items():
        pattern = tuple(headings['h2'][:5])  # å‰5ä¸ªH2
        if len(pattern) >= 3:
            h2_patterns[pattern] += 1

    report.append("### 2.1 æœ€å¸¸è§çš„ç»“æ„æ¨¡æ¿\n\n")
    for i, (pattern, count) in enumerate(h2_patterns.most_common(5), 1):
        report.append(f"**æ¨¡æ¿{i}** (ä½¿ç”¨æ¬¡æ•°: {count}ç¯‡)\n")
        report.append("```\n")
        for h2 in pattern:
            report.append(f"H2: {h2}\n")
        report.append("```\n\n")

    # é«˜åº¦ç›¸ä¼¼æ–‡ç« å¯¹
    report.append("### 2.2 é«˜åº¦ç›¸ä¼¼æ–‡ç« å¯¹ (ç›¸ä¼¼åº¦>75%)\n\n")
    similar_pairs = []
    checked = set()
    for r1 in results:
        for r2 in results:
            if r1['article']['slug'] != r2['article']['slug']:
                pair = tuple(sorted([r1['article']['slug'], r2['article']['slug']]))
                if pair not in checked:
                    checked.add(pair)
                    sim = calculate_similarity(
                        r1['article']['headings']['h2'],
                        r2['article']['headings']['h2']
                    )
                    if sim > 75:
                        similar_pairs.append((pair, sim))

    similar_pairs.sort(key=lambda x: x[1], reverse=True)
    for (slug1, slug2), sim in similar_pairs[:10]:
        report.append(f"- `{slug1}` â†” `{slug2}` - ç›¸ä¼¼åº¦ **{sim}%**\n")

    if not similar_pairs:
        report.append("âœ… æœªå‘ç°é«˜åº¦ç›¸ä¼¼çš„æ–‡ç« å¯¹\n")
    report.append("\n")

    # ä¸‰ã€Topic Cluster åˆ†æ
    report.append("## ä¸‰ã€æŒ‰ Topic Cluster è´¨é‡åˆ†æ\n\n")
    cluster_stats = defaultdict(list)
    for r in results:
        cluster = r['article']['topicCluster'] or 'æœªåˆ†ç±»'
        cluster_stats[cluster].append(r)

    for cluster, cluster_results in sorted(cluster_stats.items()):
        count = len(cluster_results)
        avg = sum(r['result']['total'] for r in cluster_results) / count
        best = max(cluster_results, key=lambda x: x['result']['total'])
        worst = min(cluster_results, key=lambda x: x['result']['total'])

        report.append(f"### {cluster} ({count}ç¯‡)\n\n")
        report.append(f"- å¹³å‡åˆ†: {avg:.1f}/100\n")
        report.append(f"- æœ€ä½³: `{best['article']['slug']}` ({best['result']['total']}åˆ†)\n")
        report.append(f"- æœ€å·®: `{worst['article']['slug']}` ({worst['result']['total']}åˆ†)\n\n")

    # å››ã€TOP/BOTTOM 10
    report.append("## å››ã€TOP 10 ä¼˜ç§€æ–‡ç« \n\n")
    report.append("| æ’å | æ–‡ç«  | æ€»åˆ† | SEO | å†…å®¹ | å…³é”®è¯ | UX | å·®å¼‚åŒ– | AIæ‰£åˆ† |\n")
    report.append("|------|------|------|-----|------|--------|----|---------|---------|\n")

    for i, r in enumerate(results[:10], 1):
        s = r['result']['scores']
        report.append(f"| {i} | `{r['article']['slug'][:40]}...` | {r['result']['total']} | "
                     f"{s['seo_tech']} | {s['content']} | {s['keyword']} | {s['ux']} | "
                     f"{s['differentiation']} | -{s['ai_penalty']} |\n")

    report.append("\n## äº”ã€BOTTOM 10 é—®é¢˜æ–‡ç« \n\n")
    report.append("| æ’å | æ–‡ç«  | æ€»åˆ† | ä¸»è¦é—®é¢˜ |\n")
    report.append("|------|------|------|----------|\n")

    for i, r in enumerate(results[-10:], 1):
        issues = ', '.join(r['result']['issues'][:2])
        report.append(f"| {i} | `{r['article']['slug'][:40]}...` | {r['result']['total']} | {issues} |\n")

    # å…­ã€AIç—•è¿¹åˆ†æ
    report.append("\n## å…­ã€AI ç—•è¿¹åˆ†æ\n\n")

    all_ai_markers = Counter()
    for r in results:
        for marker, count in r['result']['ai_markers']:
            all_ai_markers[marker] += count

    report.append("### é«˜é¢‘ AI ç‰¹å¾è¯ç»Ÿè®¡\n\n")
    for marker, count in all_ai_markers.most_common(10):
        report.append(f"- `\"{marker}\"` - å‡ºç° {count} æ¬¡\n")

    report.append("\n### AI é«˜é£é™©æ–‡ç«  (æ‰£åˆ†â‰¥5åˆ†)\n\n")
    for r in [r for r in results if r['result']['scores']['ai_penalty'] >= 5]:
        report.append(f"- `{r['article']['slug']}` - AIæ‰£åˆ†: {r['result']['scores']['ai_penalty']}åˆ†\n")

    # ä¸ƒã€ä¼˜åŒ–å»ºè®®
    report.append("\n## ä¸ƒã€ä¼˜åŒ–ä¼˜å…ˆçº§å»ºè®®\n\n")

    report.append("### ğŸ”¥ é«˜ä¼˜å…ˆçº§ (Quick Wins)\n\n")
    report.append(f"1. **è¡¥å……å†…é“¾** - {len(no_internal_links)}ç¯‡æ–‡ç« ç¼ºå°‘ relatedSlugs\n")

    no_keyword_in_title = [r for r in results if r['article']['primaryKeyword']
                           and r['article']['primaryKeyword'].lower() not in r['article']['title'].lower()]
    report.append(f"2. **æ ‡é¢˜ä¼˜åŒ–** - {len(no_keyword_in_title)}ç¯‡æ ‡é¢˜æœªåŒ…å«ä¸»å…³é”®è¯\n")

    no_faq = [r for r in results if 'faq' not in r['article']['content'].lower()]
    report.append(f"3. **æ·»åŠ  FAQ** - {len(no_faq)}ç¯‡æ–‡ç« ç¼ºå°‘ FAQ éƒ¨åˆ†\n\n")

    report.append("### âš ï¸ ä¸­ä¼˜å…ˆçº§ (éœ€æ·±åº¦æ”¹å†™)\n\n")
    high_similarity = [r for r in results if r['result']['avg_similarity'] > 70]
    report.append(f"1. **é™ä½ç»“æ„åŒè´¨åŒ–** - {len(high_similarity)}ç¯‡æ–‡ç« ç»“æ„ç›¸ä¼¼åº¦è¿‡é«˜\n")
    report.append(f"2. **å‡å°‘ AI ç—•è¿¹** - {len(high_ai)}ç¯‡æ–‡ç«  AI ç‰¹å¾æ˜æ˜¾\n\n")

    report.append("### ğŸ“Š ä½ä¼˜å…ˆçº§ (é•¿æœŸä¼˜åŒ–)\n\n")
    report.append("1. å¢åŠ å›¾ç‰‡å’Œè§†è§‰å…ƒç´  (æ‰€æœ‰æ–‡ç«  image å­—æ®µä¸ºç©º)\n")
    report.append("2. æ‰©å……æ–‡ç« é•¿åº¦ (éƒ¨åˆ†æ–‡ç« <1000è¯)\n")

    # å†™å…¥æ–‡ä»¶
    with open('article-seo-analysis-report.md', 'w', encoding='utf-8') as f:
        f.write(''.join(report))

def generate_csv(results):
    """ç”Ÿæˆ CSV è¯„åˆ†è¡¨"""
    import csv

    with open('article-seo-scores.csv', 'w', newline='', encoding='utf-8') as f:
        writer = csv.writer(f)
        writer.writerow([
            'slug', 'æ€»åˆ†', 'SEOæŠ€æœ¯åˆ†', 'å†…å®¹è´¨é‡åˆ†', 'å…³é”®è¯ä¼˜åŒ–åˆ†',
            'ç”¨æˆ·ä½“éªŒåˆ†', 'å·®å¼‚åŒ–åˆ†', 'AIç—•è¿¹æ‰£åˆ†', 'ç»“æ„ç›¸ä¼¼åº¦%', 'ä¸»è¦é—®é¢˜', 'ä¼˜åŒ–å»ºè®®'
        ])

        for r in results:
            s = r['result']['scores']
            issues = '; '.join(r['result']['issues'][:2])
            suggestion = "è¡¥å……å†…é“¾" if not r['article']['relatedSlugs'] else "ä¼˜åŒ–å…³é”®è¯å¯†åº¦"

            writer.writerow([
                r['article']['slug'],
                r['result']['total'],
                s['seo_tech'],
                s['content'],
                s['keyword'],
                s['ux'],
                s['differentiation'],
                s['ai_penalty'],
                f"{r['result']['avg_similarity']:.1f}",
                issues,
                suggestion,
            ])

if __name__ == '__main__':
    main()
